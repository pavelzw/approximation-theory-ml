\documentclass[12pt]{article}

\input{preamble}

\usepackage[margin=2cm]{geometry}

\usepackage{mathtools}

\newcommand{\N}{\mathbb{N}} % blackboard bold N for natural numbers
\newcommand{\R}{\mathbb{R}} % blackboard bold R for real numbers
\newcommand{\Z}{\mathbb{Z}} % blackboard bold Z for integers

\renewcommand{\set}[1]{\{#1\}}

\begin{document}

{\Large Proposition III.2}

Wir zeigen folgende Behauptung für die lineare Interpolation 
\(I_m : [0,1] \rightarrow \R, m\in \N\) der Funktion 
\(F(x) = x - x^2\) an den Punkten \(\frac{j}{2^m}, j \in \set{0,\ldots, 2^m}\).
Für jedes \(m\in\N\) ist der Interpolationsfehler identisch auf jedem Intervall zwischen 
den Interpolationspunkten. \textit{Zeige Fig. 2 aus Paper an Beamer}

\begin{center}
    \includegraphics[width=5cm]{images/iii2_1.jpg} % iii2_1
\end{center}

Sei deshalb \(f_m: [0,2^{-m}] \rightarrow [0,2^{-2m-2}]\) definiert als \(f_m(x) = 2^{-m}x - x^2\)
und dessen lineare Interpolation \(h_m: [0,2^{-m}] \rightarrow [0,2^{-2m-2}]\) mit 
\[ h_m(x) \coloneqq \begin{cases}
    2^{-m-1}x, &x\in [0,2^{-m-1}), \\
    -2^{-m-1}x + 2^{-2m-1}, &x \in [2^{-m-1}, 2^{-m}].
\end{cases} \]

\begin{center}
    \includegraphics[width=5cm]{images/iii2_2.jpg} % iii2_2
\end{center}

Direktes Nachrechnen zeigt, dass 
\[ f_m(x) - h_m(x) = \begin{cases}
    f_{m+1}(x), &x\in [0, 2^{-m-1}), \\
    f_{m+1}(x - 2^{-m-1}), &x \in [2^{-m-1}, 2^{-m}].
\end{cases} \]

\textit{Zeige Fig. 2 aus Paper an Beamer}

\(F = f_0\) und \(I_1 = h_0\) implizieren, dass 
\[ F(x) - I_m(x) = f_m(x - \frac{j}{2^m}) \quad x\in [\frac{j}{2^m}, \frac{j+1}{2^m}] \]
und \(I_m = \sum_{k=0}^{m-1} H_k\) mit \(H_k : [0,1]\rightarrow \R\) gegeben durch 
\[ H_k(x) = h_k(x - \frac{j}{2^k}), \quad x\in [\frac{j}{2^m}, \frac{j+1}{2^m}]. \]

\begin{center}
    \includegraphics[width=5cm]{images/iii2_3.jpg} % iii2_3
\end{center}

Daraus folgt 
\[ \sup_{x\in [0,1]} |x^2 - (x - I_m(x)) | = \sup_{x\in [0,1]} |F(x) - I_m(x) | = \sup_{x\in [0, 2^{-m}]} |f_m(x)| = 2^{-2m-2}. \]

\textit{Hier verwenden wir das vorherige Lemma, 
der Approximationsfehler bleibt gleich in jedem Intervall.}

Aus der Sawtooth Konstruktion folgt, dass wir eine einfache Approximation der \(H_k\) finden 
durch ein neuronales Netzwerk mit zwei Neuronen pro Ebene 
sowie einem dritten Neuron, um die Approximation \(x - I_m(x)\) für \(x^2\) zu erreichen.

Sei \(s_k(x) \coloneqq 2^{-1}\rho(x) - \rho(x - 2^{-2k-1})\).
\begin{center}
    \includegraphics[width=5cm]{images/iii2_4.jpg} % iii2_4
\end{center}

\textit{Nun können wir jede dieser \(H_k\) Summen durch eine Netz Ebene approximieren 
und kommen so auf einen Fehler, der exponentiell schrumpft in der Netzwerklänge.}

\textit{Evtl. überspringen?}

Es gilt \(H_k = s_k \circ H_{k-1}\). Wir können ein Netzwerk \(x - I_m(x)\) wie folgt definieren: 
\(A_1 \coloneqq (1, 1, 1)^T \in \R^{3\times 1}, b_1 \coloneqq (0, -2^{-1}, 0)^T \in \R^3 \),
\[ A_l \coloneqq \begin{pmatrix}
    2^{-1} & -1 & 0 \\ 2^{-1} & -1 & 0 \\ -2^{-1} & 1 & 1
\end{pmatrix} \in \R^{3 \times 3}, b_l \coloneqq \begin{pmatrix}
    0 \\ -2^{-2l + 1} \\ 0
\end{pmatrix} \in \R^3 \]
sowie 
\(A_{m+1} \coloneqq (-2^{-1}, 1, 1) \in \R^{1 \times 3}, b_{m+1} = 0\).

Setzen wir \(W_l(x) \coloneqq A_l x + b_l\) und 
\[ \tilde{\Phi}_m \coloneqq W_{m+1} \circ \rho \circ W_m \circ \cdots \circ \rho \circ W_1, \]
bekommen wir durch direktes Nachrechnen \(\tilde{\Phi}_m(x) = x - \sum_{k=0}^{m-1} H_k(x)\). 
Der Fehler schrumpft exponentiell in der Netzwerklänge, \(\Phi_\varepsilon \coloneqq \tilde{\Phi}_{\lceil \log(\varepsilon^{-1})/2 \rceil}\) 
erfüllt die geforderten Eigenschaften.

\newpage
{\Large Proposition III.3}

Wir können o. B. d. A. annehmen, dass \(D \geq 1\) gilt. Für \(D < 1\) 
betrachten wir einfach das Netz, das für \(D=1\) konstruiert wird, welche die gleichen 
Eigenschaften erfüllt.

Die Argumentation basiert auf der Polarisationsformel 
\[ xy = \frac{1}{4} ((x+y)^2 - (x-y)^2). \]
Wir konstruieren zwei Quadrier-Netzwerke (Proposition III.2), 
die die Neuronen, die für die Summation der \(H_k\) verwenden, teilen 
sowie davor ein Layer, welches \((x,y)\) auf 
\((\frac{|x+y|}{2D}, \frac{|x-y|}{2D})\) 
abbilded sowie eine Ebene, die mit \(D^2\) multipliziert mit Gewichten, 
die durch \(1\) beschränkt sind. 
\textit{Den extra Schritt mit dem um \(D\) skalieren und zum Schluss wieder reskalieren 
machen wir, damit wir das Quadrierungsnetzwerk aus dem vorherigen Satz verwenden können.}

\textit{Matrizen auf den Folien kurz zeigen}

Baue hier ein Netzwerk \(\tilde{\Psi}_m\), 
welches folgendes erfüllt: 
\[ \tilde{\Psi}_m(x,y) = \tilde{\Phi}_m(\frac{|x+y|}{2D}) - \tilde{\Phi}_m(\frac{|x-y|}{2D}), \qquad(*) \]
wobei \(\tilde{\Phi}_m\) wie im vorherigen Beweis definiert ist.

Nun folgt 
\[ \sup_{(x,y) \in [-D,D]^2} |\tilde{\Psi}_m(x,y) - \frac{xy}{D^2} | 
= \sup_{(x,y) \in [-D,D]^2} |(\tilde{\Phi}_m(\frac{|x+y|}{2D}) - \tilde{\Phi}_m(\frac{|x-y|}{2D})) 
- ((\frac{|x+y|}{2D})^2 - (\frac{|x-y|}{2D})^2)| \] 
\[ \leq 2 \sup_{z\in [0,1]} |\tilde{\Phi}_m(z) - z^2| \leq 2^{-2m-1}. \qquad (**)\]

Setze nun \(\Psi_D(x) = D^2 x\) und wähle 
\(\Phi_{D,\varepsilon} \coloneqq \Psi_D \circ \tilde{\Psi}_{m(D,\varepsilon)} \), 
wobei \(m(D, \varepsilon) \coloneqq \lceil 2^{-1}(1+\log(D^2 \varepsilon^{-1})) \rceil \).
Dann folgt die Fehlerabschätzung aus \((**)\) und die geforderten Beschränkungen an 
die Breite, Länge und das Gewicht folgen aus der Konstruktion des Netzes zusammen mit Proposition III.2.

Weiter folgt \(\Phi_{D,\varepsilon}(0,x) = \Phi_{D,\varepsilon}(x,0) = 0\) direkt aus \((*)\). \qed

\textit{Jetzt haben wir ein Netzwerk konstruiert, welches Gewichte hat, die durch \(1\) 
beschränkt sind, egal wie groß \(D\) ist.}

\newpage

{\Large Lemma III.7}

\textit{Es gibt ein fundamentales Ergebnis aus der Lagrange Interpolation mit 
Tschebyscheff Punkten, welche folgendes garantiert.}
Satz: 
Für alle \(f\in \mathcal{S}_{[-1,1]}\), \(m\in\N\) existiert ein Polynom 
\(P_{f,m}\) mit Grad \(m\), sodass 
\[ ||f - P_{f,m}||_{L^\infty ([-1,1])} \leq \frac{1}{(m+1)!2^m} ||f^{(m+1)}||_{L^\infty ([-1,1])} \leq \frac{1}{2^m}. \]

Wir können \(P_{f,m}\) durch die Tschebyscheff-Basis darstellen: 
\[ P_{f,m} = \sum_{j=0}^m c_{f,m,j} T_j(x) \]
mit \(|c_{f,m,j}| \leq 2\) 
und den Tschebyscheff-Polynomen, die durch die Zweitermrekursion 
\[ T_k(x) = 2x T_{k-1}(x) - T_{k-2}(x), T_0(x) = 1, T_1(x) = x \]
definiert sind.
Weiter gilt, dass die Koeffizienten von \(T_k\) in der Monom-Basis 
durch \(3^k\) beschränkt sind. 
Also können wir \(P_{f,m} = \sum_{j=0}^m a_{f,m,j} x^j\) schreiben mit 
\[ A_{f,m} \coloneqq \max_{j=0,\ldots, m} |a_{f,m,j}| \leq 2(m+1)3^m. \] 

Wenn wir nun Proposition III.5 anwenden auf \(P_{f,m}\) mit \(m= \lceil \log(2/\varepsilon)\rceil\) 
und Approximationsfehler \(\varepsilon/2\), folgt die Behauptung durch
\[ C' m(\log(2/\varepsilon) + \log(m) + \log(|A_{f,m}|)) \leq C (\log(\varepsilon^{-1}))^2 \]
für eine Konstante \(C\). \qed

\newpage

{\Large Theorem III.8}

Es gilt \(f(x) := (6/\pi^3) \cos(\pi x) \in \mathcal{S}_{[-1,1]}\). Also existiert nach Lemma III.7 
ein \(C>0\), sodass für jedes \(\varepsilon \in (0,1/2)\) ein Netzwerk \(\Phi_\varepsilon \in \mathcal{N}_{1,1}\) 
mit \(\mathcal{L}(\Phi_\varepsilon) \leq C \log(\varepsilon^{-1})^2, \mathcal{W}(\Phi_\varepsilon) \leq 9, 
\mathcal{B}(\Phi_\varepsilon) \leq 1\) sowie 
\[ ||\Phi_\varepsilon - f ||_{L^{\infty}([-1,1])} \leq \frac{6}{\pi^3} \varepsilon. \tag{\(*\)} \]

\textit{Wir erweitern dieses Resultat jetzt für die Approximatino von \(\cos(ax)\) auf \([-1,1]\) für beliebiges \(a\in \R_+\). 
Das bekommen wir hin, indem wir ausnutzen, dass \(\cos(\pi x))\) \(2\)-periodisch und gerade ist.}

Sei \(g_s:[0,1] \rightarrow [0,1]\) die Sawtooth Funktion der Ordnung \(s\) und aufgrund der Periodizität und Symmetrie der Cosinus Funktion gilt 
\[ \cos(\pi 2^s x) = \cos(\pi g_s(|x|)). \]

Sei \(a > \pi\). Definiere \(s = s(a) \coloneqq \lceil \log(a) - \log(\pi) \rceil\) und \(\alpha = \alpha(a) \coloneqq (\pi 2^s)^{-1}a \in (1/2, 1]\). 
Es gilt dann 
\[ \cos(ax) = cos(\pi 2^s \alpha x) = \cos(\pi g_s(\alpha |x|)). \]

Es folgt aus \((*)\)
\[ || \frac{\pi^3}{6} \Phi_\varepsilon (g_s(\alpha |x|)) - \cos(ax) ||_{L^{\infty}([-1,1])} = \frac{\pi^3}{6} ||\Phi_\varepsilon (g_s(\alpha |x|)) - f(g_s(\alpha |x|))||_{L^\infty ([-1,1])} \leq \varepsilon. \]

\textit{Wir wollen nun dieses Netzwerk \(\frac{\pi^3}{6} \Phi_\varepsilon (g_s(\alpha |x|))\) realisieren.}
Erstelle Netzwerke \(\Psi_g^s(x) = g_s(x)\) mit \(\mathcal{B}(\Psi_g^s) \leq 1, \mathcal{L}(\Psi_g^s ) = 7(s+1)\) und \(\mathcal{W}(\Psi_g^s) = 3\).
\textit{Das geht, indem wir die Netzwerke \(\Phi_g^s\), die zu Beginn des Vortrags definiert worden sind, die \(g_s\) darstellen, etwas anpassen.}
Setze weiter \(\Phi(x) \coloneqq \alpha \rho(x) - \alpha \rho(-x) = \alpha |x|\) 
und nimm ein Multiplikationsnetzwerk \(\Phi_{\pi^3/6}^{\text{mult}}\). Setze nun 
\[ \Psi_{a,\varepsilon} \coloneqq \Phi_{\pi^3/6}^{\text{mult}} \circ \Phi_\varepsilon \circ \Psi_g^s \circ \Psi = \Phi_\varepsilon(g_s(\alpha |x|)). \]
\textit{Aus dem Lemma über Konkatenationen von neuronalen Netzen folgt nun:}
\(\Rightarrow \mathcal{L}(\Psi_{a,\varepsilon}) \leq C ((\log(\varepsilon^{-1}))^2 + \log(\lceil a \rceil) ), \mathcal{W}(\Psi_{a,\varepsilon}) \leq 9\) 
und \( \mathcal{B}(\Psi_{a,\varepsilon}) \leq 1 \). 
\(\Rightarrow\) gewünschte Apprximation für \(a > \pi\).

Für \(a < \pi\), wähle 
\[ \Psi_{a,\varepsilon} \coloneqq \Phi_{\pi^3/6}^{\text{mult}} \circ \Phi_\varepsilon,  \]
da \(x\mapsto (6/\pi^3) \cos(ax) \in \mathcal{S}_{[-1,1]}\).

Betrachte nun für ein beliebiges \(D\geq 1\) die Approximation von \(x \mapsto \cos(ax)\) auf \([-D,D]\). 
Definiere \(\Psi_{a,D,\varepsilon}(x) \coloneqq \Psi_{aD,\varepsilon}(\frac{x}{D})\). \textit{[Achtung, einmal im Index \(a,D\), einmal \(aD\)]} Wir sehen 
\begin{align*}
    \sup_{x\in [-D,D]} |\Psi_{a,D,\varepsilon}(x) - \cos(ax)| &= \sup_{y\in [-1,1]} |\Psi_{a,D,\varepsilon} (Dy) - \cos(aDy) | \\
    &= \sup_{y\in [-1,1]} |\Psi_{aD,\varepsilon}(y) - \cos(aDy)| \leq \varepsilon. 
\end{align*}\qed

\newpage

{\Large Korollar III.9}

Seien \(a,D\in \R_+, b\in \R, \varepsilon \in (0, 1/2)\) gegeben. 
Das Netzwerk 
\[ \Psi_{a,b,D,\varepsilon}(x) \coloneqq \Psi_{a, D + \frac{|b|}{a}, \varepsilon}(x - \frac{b}{a}) \]
erfüllt die geforderten Bedingungen: 
\[ \sup_{x\in [-D,D]} |\Psi_{a,b,D,\varepsilon}(x) - \cos(ax-b) | \leq \sup_{y \in [-(D+\frac{|b|}{a}), D+ \frac{|b|}{a}]} |\Psi_{a,D+\frac{|b|}{a},\varepsilon}(y) - \cos(ay)| \leq \varepsilon. \]
\textit{[Durch die Substitution \(y = x + \frac{b}{a}\)]}

\end{document}